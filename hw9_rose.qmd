---
title: "Homework #9: Stacking and Boosting" 
author: "**Rose Eluvathingal Muttikkal**"
format: ds6030hw-html
---


```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

```{r}
library(dplyr)
library(purrr)
library(rpart) 
library(rpart.plot)
library(ipred)
```



# Problem 1: Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model. 

- You will need to register in Kaggle (its free)
- Read the details of the contest. Understand the data and evaluation function.
- Make at least one submission that uses **stacking or model averaging**. 
- If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points. 
    - I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions. 
    - You don't need to team, but must still combine multiple models. At least one of the component models should be boosting. 
- Each person submit the following in Canvas:
    - Code (if teaming, your code and the shared stacking code)
    - kaggle name (or team name) so we can ensure you had a valid submission. 
    - your score and current ranking on the kaggle leaderboard
- Top 5 scores get 2 bonus points
    - Teams will split their bonus points among team members

# Data Prep
    
```{r}
train_path <- "/Users/rosee.m./Desktop/bookmarks/ds_stat_learn/StatLearning_HW9/house-prices-advanced-regression-techniques/train.csv"
test_path <- "/Users/rosee.m./Desktop/bookmarks/ds_stat_learn/StatLearning_HW9/house-prices-advanced-regression-techniques/test.csv"

# load data
train <- read.csv(train_path)
test <- read.csv(test_path)
test[is.na(test)] <- 0

#(tree don't need one-hot encoding right?)
#train <- data.frame(lapply(train, function(x) {
 # if(is.character(x)) as.numeric(as.factor(x)) else x
#}))

# set NAs to 0
train[is.na(train)] <- 0
numeric_columns <- sapply(train, is.numeric) & names(train) != "SalePrice"

# Scale numeric variables
train[numeric_columns] <- scale(train[numeric_columns])

head(train,10)
```

```{r}
set.seed(6030) # for reproducible results
train_indices <- sample(1:nrow(train), 0.8 * nrow(train))
train_data <- train[train_indices, ]
test_data <- train[-train_indices, ]
```

# EDA
```{r}
# Assuming 'df' is your data frame and 'your_variable' is the variable you want to plot
ggplot(train_data, aes(x = SalePrice)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sale Price",
       x = "Sale Price",
       y = "Frequency") +
  theme_minimal()

ggplot(train_data, aes(x = log(SalePrice))) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Log Sale Price",
       x = "Log Sale Price",
       y = "Frequency") +
  theme_minimal()
```
Set up x and y
```{r}
#Goal is to predict the log salary 
train_data <- train_data %>% 
  mutate(SalePrice=log(SalePrice)) %>% 
  rename(Y=SalePrice)#convert to logSalary 

test_data <- test_data %>% 
  mutate(SalePrice=log(SalePrice)) %>% #convert to logSalary 
  rename(Y=SalePrice) 

#train data

#test data 
X.test <- test_data %>% 
  select(-Y)
  
Y.test <- test_data %>% 
  select(Y)  
```

Run tree
```{r}
tree=rpart(Y~.,data=train_data) 
summary(tree,cp=1)
length(unique(tree$where))

# plot tree
prp(tree,type=1,extra=1,branch=1)
```







