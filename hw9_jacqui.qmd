---
title: "Homework #9: Stacking and Boosting" 
author: "**Jacqui Unciano**"
format: ds6030hw-html
---


```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Problem 1: Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model. 

- You will need to register in Kaggle (its free)
- Read the details of the contest. Understand the data and evaluation function.
- Make at least one submission that uses **stacking or model averaging**. 
- If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points. 
    - I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions. 
    - You don't need to team, but must still combine multiple models. At least one of the component models should be boosting. 
- Each person submit the following in Canvas:
    - Code (if teaming, your code and the shared stacking code)
    - kaggle name (or team name) so we can ensure you had a valid submission. 
    - your score and current ranking on the kaggle leaderboard
- Top 5 scores get 2 bonus points
    - Teams will split their bonus points among team 
    
```{r}
#packages
library(tidyverse)
library(glmnet)
```
    
```{r}
#train_path = "C:/jacqu/OneDrive/Documents/MSDS/datasets/train.csv"
#test_path = "C:/Users/jacqu/OneDrive/Documents/MSDS/datasets/test.csv"
train = read.csv("C:/Users/jacqu/OneDrive/Documents/MSDS/datasets/train.csv")
test = read.csv("C:/Users/jacqu/OneDrive/Documents/MSDS/datasets/test.csv")
train[is.na(train)] = 0
head(train)
test[is.na(test)] = 0
head(test)
```

```{r}
set.seed(6030) # for reproducible results
train_indices = sample(1:nrow(train), 0.8 * nrow(train))
train_data = train[train_indices, ]
test_data = train[-train_indices, ]
```

```{r}
mx = glmnet::makeX(
  train = train_data %>% select(-SalePrice),
  test = test_data %>% select(-SalePrice))

mx.train = mx$x
y.train = train_data %>% pull(SalePrice)
mx.test = mx$xtest

nfolds = 10
folds = sample(rep(1:nfolds, length=nrow(mx.train)))

alpha = seq(0,1,0.01)
tab = data.frame(matrix(ncol=3,nrow=0, 
                        dimnames=list(NULL, c("alpha", "lambda", 
                                              "mse")))) 

for(a in alpha){
  mod = cv.glmnet(mx.train,
                  y.train,
                  alpha = a,
                  foldid = folds)
  best_mse = min(mod$cvm)
  tab = rbind(tab, list(alpha=a, lambda=mod$lambda.min, 
                            mse=best_mse))
  
}

best_alpha = tab %>%
  arrange(mse) %>%
  slice(1) %>%
  pull(alpha)

best_lambda = tab %>%
  arrange(mse) %>%
  slice(1) %>%
  pull(lambda)

enet_model = glmnet(mx.train,
                       y.train,
                       alpha = best_alpha,
                       lambda = best_lambda)

preds = predict(enet_model, mx.test, s="lambda.min")
# preds$yhat = preds$lambda.min
# 
# predictions = preds %>% 
#   select(-1)
# 
rmse = sqrt(mean((log(preds) - log(test_data$SalePrice))^2))
print(paste("RMSE:", rmse))
```

