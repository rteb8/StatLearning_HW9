---
title: "Homework #9: Stacking and Boosting" 
author: "Skye Jung"
format: ds6030hw-html
---


```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Problem 1: Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model. 

- You will need to register in Kaggle (its free)
- Read the details of the contest. Understand the data and evaluation function.
- Make at least one submission that uses **stacking or model averaging**. 
- If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points. 
    - I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions. 
    - You don't need to team, but must still combine multiple models. At least one of the component models should be boosting. 
- Each person submit the following in Canvas:
    - Code (if teaming, your code and the shared stacking code)
    - kaggle name (or team name) so we can ensure you had a valid submission. 
    - your score and current ranking on the kaggle leaderboard
- Top 5 scores get 2 bonus points
    - Teams will split their bonus points among team members
```{r}
# load data
train <- read.csv('/Users/skyejung/Desktop/DS 6030/train.csv')
test <- read.csv('/Users/skyejung/Desktop/DS 6030/test.csv')
test[is.na(test)] <- 0

# Assuming 'data' is your dataframe

library(dplyr)
library(purrr)

train <- data.frame(lapply(train, function(x) {
  if(is.character(x)) as.numeric(as.factor(x)) else x
}))


# set NAs to 0
train[is.na(train)] <- 0

head(train,10)
```

```{r}
set.seed(6030) # for reproducible results
train_indices <- sample(1:nrow(train), 0.8 * nrow(train))
train_data <- train[train_indices, ]
test_data <- train[-train_indices, ]
```

```{r}
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(train_data[,-which(names(train_data) == "SalePrice")]), label = train_data$SalePrice)
dtest <- xgb.DMatrix(data = as.matrix(test_data[,-which(names(test_data) == "SalePrice")]), label = test_data$SalePrice)
```

```{r}
# Set parameters for xgboost
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 3,
  min_child_weight = 1,
  subsample = 0.5,
  colsample_bytree = 0.5
)

# Number of boosting rounds
nrounds <- 100

# Train the model
model <- xgb.train(params = params, data = dtrain, nrounds = nrounds)

# Predict on test data
predictions <- predict(model, dtest)

# Calculate RMSE
rmse <- sqrt(mean((log(predictions) - log(test_data$SalePrice))^2))
print(paste("RMSE:", rmse))
```


