---
title: "Homework #9: Stacking and Boosting" 
author: "**Rose Eluvathingal Muttikkal, Skye Jung, Grace Zhang, Serene Lu, Jacqui Unciano, Isha Thrukal**"
format: ds6030hw-html
---


```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Problem 1: Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model. 

- You will need to register in Kaggle (its free)
- Read the details of the contest. Understand the data and evaluation function.
- Make at least one submission that uses **stacking or model averaging**. 
- If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points. 
    - I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions. 
    - You don't need to team, but must still combine multiple models. At least one of the component models should be boosting. 
- Each person submit the following in Canvas:
    - Code (if teaming, your code and the shared stacking code)
    - kaggle name (or team name) so we can ensure you had a valid submission. 
    - your score and current ranking on the kaggle leaderboard
- Top 5 scores get 2 bonus points
    - Teams will split their bonus points among team members

```{r}
library(dplyr)
library(purrr)
library(rpart) 
library(rpart.plot)
library(ipred)
library(ggplot2)
library (readr)
```

### Data Prep
    
```{r}
train_path = "https://raw.githubusercontent.com/rteb8/StatLearning_HW9/main/house-prices-advanced-regression-techniques/train.csv"
test_path = "https://raw.githubusercontent.com/rteb8/StatLearning_HW9/main/house-prices-advanced-regression-techniques/test.csv"

# load data
train <- read.csv(url(train_path))
test <- read.csv(url(test_path))
test[is.na(test)] <- 0

numeric_columns <- sapply(train, is.numeric) & names(train) != "SalePrice"

# Scale numeric variables
train[numeric_columns] <- scale(train[numeric_columns])

train <- data.frame(lapply(train, function(x) {
  if(is.character(x)) as.numeric(as.factor(x)) else x
}))


# set NAs to 0
train[is.na(train)] <- 0

head(train,10)

## Test
numeric_columns <- sapply(test, is.numeric) & names(test) != "SalePrice"

# Scale numeric variables
test[numeric_columns] <- scale(test[numeric_columns])

test <- data.frame(lapply(test, function(x) {
  if(is.character(x)) as.numeric(as.factor(x)) else x
}))


# set NAs to 0
test[is.na(test)] <- 0
```

```{r}
#Goal is to predict the log salary 
train <- train %>% 
  mutate(SalePrice=log(SalePrice)) %>% 
  rename(Y=SalePrice)#convert to logSalary 
```

### Bagging Tree Model (Rose)
```{r}
set.seed(2019)
bag_tree <- bagging(
  formula = Y ~ .,
  data = train,
  nbagg = 146,  
  coob = TRUE )
bag_tree
```

```{r}
Z1_tree = predict(bag_tree, test)
```



### Random Forest (Serene)
```{r}
library(ranger)
set.seed(6030)
g1 = ranger(Y ~ ., data = train)
Z2_rf = predict(g1, test)$predictions
```

### GLM (Grace)
```{r}
library(glmnet)
X = makeX(train = train %>% select(-Y),
           test = test)
set.seed(2023)
g2 = cv.glmnet(X$x, train$Y) # tune lambda with 10-fold cv
Z3_lr = predict(g2, X$xtest, s = "lambda.min")
```

### Xgboost (Skye)
```{r}
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(train[,-which(names(train) == "Y")]), label = train$Y)

# Set parameters for xgboost
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 3,
  min_child_weight = 1,
  subsample = 0.5,
  colsample_bytree = 0.5
)

# Number of boosting rounds
nrounds <- 100

# Train the model
boost_model <- xgb.train(params = params, data = dtrain, nrounds = nrounds)
```

```{r}
dtest <- xgb.DMatrix(data = as.matrix(test))

# Predict on test data
Z4_boost <- predict(boost_model, dtest)
```

### Elastic Net (Jacqui)
```{r}
library(glmnet)
library(dplyr)
mx = glmnet::makeX(
  train = train %>% select(-Y),
  test = test)

mx.train = mx$x
y.train = train %>% pull(Y)
mx.test = mx$xtest

nfolds = 10
folds = sample(rep(1:nfolds, length=nrow(mx.train)))

alpha = seq(0,1,0.01)
tab = data.frame(matrix(ncol=3,nrow=0, 
                        dimnames=list(NULL, c("alpha", "lambda", 
                                              "mse")))) 

for(a in alpha){
  mod = cv.glmnet(mx.train,
                  y.train,
                  alpha = a,
                  foldid = folds)
  best_mse = min(mod$cvm)
  tab = rbind(tab, list(alpha=a, lambda=mod$lambda.min, 
                            mse=best_mse))
  
}

best_alpha = tab %>%
  dplyr::arrange(mse) %>%
  dplyr::slice(1) %>%
  dplyr::pull(alpha)

best_lambda = tab %>%
  dplyr::arrange(mse) %>%
  dplyr::slice(1) %>%
  dplyr::pull(lambda)

enet_model = glmnet(mx.train,
                       y.train,
                       alpha = best_alpha,
                       lambda = best_lambda)

Z5_en = predict(enet_model, mx.test, s="lambda.min")
```


### Model Averaging 
```{r}
preds = (exp((Z1_tree+ Z2_rf+ Z3_lr+ Z4_boost+ Z5_en)/5))
length(preds)
preds <- cbind(Id = rownames(preds), preds)
colnames(preds) <- c('Id','SalePrice')
```

```{r}
# write.csv(preds, file = "/Users/skyejung/Desktop/DS 6030/hw9predictions",row.names = FALSE)
```






